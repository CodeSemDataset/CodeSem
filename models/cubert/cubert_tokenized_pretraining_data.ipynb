{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Create masked LM/next sentence masked_lm TF examples for BERT.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.flags\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_string(\"input_file\", None,\n",
    "                    \"Input raw text file (or comma-separated list of files).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_file\", None,\n",
    "    \"Output TF example file (or comma-separated list of files).\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", None,\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_whole_word_mask\", False,\n",
    "    \"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_seq_length\", 128, \"Maximum sequence length.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_predictions_per_seq\", 20,\n",
    "                     \"Maximum number of masked LM predictions per sequence.\")\n",
    "\n",
    "flags.DEFINE_integer(\"random_seed\", 12345, \"Random seed for data generation.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"dupe_factor\", 10,\n",
    "    \"Number of times to duplicate the input data (with different masks).\")\n",
    "\n",
    "flags.DEFINE_float(\"masked_lm_prob\", 0.15, \"Masked LM probability.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"short_seq_prob\", 0.1,\n",
    "    \"Probability of creating sequences which are shorter than the \"\n",
    "    \"maximum length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "  \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "               is_random_next):\n",
    "    self.tokens = tokens\n",
    "    self.segment_ids = segment_ids\n",
    "    self.is_random_next = is_random_next\n",
    "    self.masked_lm_positions = masked_lm_positions\n",
    "    self.masked_lm_labels = masked_lm_labels\n",
    "\n",
    "  def __str__(self):\n",
    "    s = \"\"\n",
    "    s += \"tokens: %s\\n\" % (\" \".join(\n",
    "        [tokenization.printable_text(x) for x in self.tokens]))\n",
    "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "        [str(x) for x in self.masked_lm_positions]))\n",
    "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "        [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n",
    "    s += \"\\n\"\n",
    "    return s\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "  writers = []\n",
    "  for output_file in output_files:\n",
    "    writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "  writer_index = 0\n",
    "\n",
    "  total_written = 0\n",
    "  for (inst_index, instance) in enumerate(instances):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "      masked_lm_positions.append(0)\n",
    "      masked_lm_ids.append(0)\n",
    "      masked_lm_weights.append(0.0)\n",
    "\n",
    "    next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "    features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writers[writer_index].write(tf_example.SerializeToString())\n",
    "    writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "    total_written += 1\n",
    "\n",
    "    if inst_index < 20:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [tokenization.printable_text(x) for x in instance.tokens]))\n",
    "\n",
    "      for feature_name in features.keys():\n",
    "        feature = features[feature_name]\n",
    "        values = []\n",
    "        if feature.int64_list.value:\n",
    "          values = feature.int64_list.value\n",
    "        elif feature.float_list.value:\n",
    "          values = feature.float_list.value\n",
    "        tf.logging.info(\n",
    "            \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "\n",
    "  for writer in writers:\n",
    "    writer.close()\n",
    "\n",
    "  tf.logging.info(\"Wrote %d total instances\", total_written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f927477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_int_feature(values):\n",
    "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_float_feature(values):\n",
    "  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ce08800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "  \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "  all_documents = []\n",
    "\n",
    "  # Input file format:\n",
    "  # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "  # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "  # sentence boundaries for the \"next sentence prediction\" task).\n",
    "  # (2) Blank lines between documents. Document boundaries are needed so\n",
    "  # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  for input_file in input_files:\n",
    "    with tf.gfile.GFile(input_file, \"r\", encoding='utf-8') as reader:\n",
    "      while True:\n",
    "        line = tokenization.convert_to_unicode(reader.readline())\n",
    "        if not line:\n",
    "          break\n",
    "        line = line.strip()\n",
    "        tokens = line.split()\n",
    "        if tokens:\n",
    "          all_documents.append(tokens)\n",
    "\n",
    "  # Remove empty documents\n",
    "  all_documents = [x for x in all_documents if x]\n",
    "  rng.shuffle(all_documents)\n",
    "\n",
    "  vocab_words = list(tokenizer.vocab.keys())\n",
    "  instances = []\n",
    "  for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "  rng.shuffle(instances)\n",
    "  return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "507d0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "  document = all_documents[document_index]\n",
    "\n",
    "  # Account for [CLS], [SEP], [SEP]\n",
    "  max_num_tokens = max_seq_length - 3\n",
    "\n",
    "  # We *usually* want to fill up the entire sequence since we are padding\n",
    "  # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "  # computation. However, we *sometimes*\n",
    "  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "  # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "  # The `target_seq_length` is just a rough target however, whereas\n",
    "  # `max_seq_length` is a hard limit.\n",
    "  target_seq_length = max_num_tokens\n",
    "  if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "  # We DON'T just concatenate all of the tokens from a document into a long\n",
    "  # sequence and choose an arbitrary split point because this would make the\n",
    "  # next sentence prediction task too easy. Instead, we split the input into\n",
    "  # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "  # input.\n",
    "  instances = []\n",
    "  current_chunk = []\n",
    "  current_length = 0\n",
    "  i = 0\n",
    "  while i < len(document):\n",
    "    segment = document[i]\n",
    "    current_chunk.append(segment)\n",
    "    current_length += len(segment)\n",
    "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "      if current_chunk:\n",
    "        # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "        # (first) sentence.\n",
    "        a_end = 1\n",
    "        if len(current_chunk) >= 2:\n",
    "          a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "          tokens_a.extend(current_chunk[j])\n",
    "\n",
    "        tokens_b = []\n",
    "        # Random next\n",
    "        is_random_next = False\n",
    "        if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "          is_random_next = True\n",
    "          target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          # This should rarely go for more than one iteration for large\n",
    "          # corpora. However, just to be careful, we try to make sure that\n",
    "          # the random document is not the same as the document\n",
    "          # we're processing.\n",
    "          for _ in range(10):\n",
    "            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "            if random_document_index != document_index:\n",
    "              break\n",
    "\n",
    "          random_document = all_documents[random_document_index]\n",
    "          random_start = rng.randint(0, len(random_document) - 1)\n",
    "          for j in range(random_start, len(random_document)):\n",
    "            tokens_b.extend(random_document[j])\n",
    "            if len(tokens_b) >= target_b_length:\n",
    "              break\n",
    "          # We didn't actually use these segments so we \"put them back\" so\n",
    "          # they don't go to waste.\n",
    "          num_unused_segments = len(current_chunk) - a_end\n",
    "          i -= num_unused_segments\n",
    "        # Actual next\n",
    "        else:\n",
    "          is_random_next = False\n",
    "          for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "        assert len(tokens_a) >= 1\n",
    "        assert len(tokens_b) >= 1\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"'[CLS]_'\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"'[SEP]_'\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(1)\n",
    "        tokens.append(\"'[SEP]_'\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "        (tokens, masked_lm_positions,\n",
    "         masked_lm_labels) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "        instance = TrainingInstance(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_random_next=is_random_next,\n",
    "            masked_lm_positions=masked_lm_positions,\n",
    "            masked_lm_labels=masked_lm_labels)\n",
    "        instances.append(instance)\n",
    "      current_chunk = []\n",
    "      current_length = 0\n",
    "    i += 1\n",
    "\n",
    "  return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e759fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"'[CLS]_'\" or token == \"'[SEP]_'\":\n",
    "      continue\n",
    "    # Whole Word Masking means that if we mask all of the wordpieces\n",
    "    # corresponding to an original word. When a word has been split into\n",
    "    # WordPieces, the first token does not have any marker and any subsequence\n",
    "    # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "    # append it to the previous set of word indexes.\n",
    "    #\n",
    "    # Note that Whole Word Masking does *not* change the training code\n",
    "    # at all -- we still predict each WordPiece independently, softmaxed\n",
    "    # over the entire vocabulary.\n",
    "    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        token.startswith(\"##\")):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "\n",
    "  rng.shuffle(cand_indexes)\n",
    "\n",
    "  output_tokens = list(tokens)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for index_set in cand_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue\n",
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if rng.random() < 0.8:\n",
    "        masked_token = \"'[MASK]_'\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if rng.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_num_tokens:\n",
    "      break\n",
    "\n",
    "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "    assert len(trunc_tokens) >= 1\n",
    "\n",
    "    # We want to sometimes truncate from the front and sometimes from the\n",
    "    # back to add more randomness and avoid biases.\n",
    "    if rng.random() < 0.5:\n",
    "      del trunc_tokens[0]\n",
    "    else:\n",
    "      trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679e93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "  input_files = []\n",
    "  for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))\n",
    "\n",
    "  tf.logging.info(\"*** Reading from input files ***\")\n",
    "  for input_file in input_files:\n",
    "    tf.logging.info(\"  %s\", input_file)\n",
    "\n",
    "  rng = random.Random(FLAGS.random_seed)\n",
    "  instances = create_training_instances(\n",
    "      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "      rng)\n",
    "\n",
    "  output_files = FLAGS.output_file.split(\",\")\n",
    "  tf.logging.info(\"*** Writing to output files ***\")\n",
    "  for output_file in output_files:\n",
    "    tf.logging.info(\"  %s\", output_file)\n",
    "\n",
    "  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                                  FLAGS.max_predictions_per_seq, output_files)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  flags.mark_flag_as_required(\"input_file\")\n",
    "  flags.mark_flag_as_required(\"output_file\")\n",
    "  flags.mark_flag_as_required(\"vocab_file\")\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
